{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(1)\n",
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "\n",
    "data = [('professor', ['you', 'smell'], 1), \n",
    "        ('professor', ['you', 'fail'], 1), \n",
    "        ('professor', ['youre', 'bad'], 1),\n",
    "        ('professor', ['above', 'average'], 0),\n",
    "        ('professor', ['hate', 'you'], 1),\n",
    "        ('professor', ['wiz', 'kid'], 0),\n",
    "        ('professor', ['amazing', 'job'], 0),\n",
    "\n",
    "        ('brother', ['great', 'job'], 1),\n",
    "        ('brother', ['wiz', 'kid'], 1),\n",
    "        ('brother', ['you', 'fail'], 0),\n",
    "        ('brother', ['hate', 'you'], 0),\n",
    "        ('brother', ['you', 'smell'], 0),\n",
    "\n",
    "        ('mom', ['you', 'smell'], 0),\n",
    "        ('mom', ['above', 'average'], 0),\n",
    "        ('mom', ['you', 'bad'], 1),\n",
    "        ('mom', ['love', 'you'], 0),\n",
    "        ('mom', ['miss', 'you'], 0),\n",
    "        ('mom', ['youre', 'disapointment'], 1),\n",
    "\n",
    "        ('sister', ['amazing', 'job'], 1),\n",
    "        ('sister', ['hate', 'you'], 0),\n",
    "        ('sister', ['miss', 'you'], 1),\n",
    "        ('sister', ['wiz', 'kid'], 1),\n",
    "        ('sister', ['love', 'you'], 0),\n",
    "\n",
    "        ('father', ['amazing', 'job'], 0),\n",
    "        ('father', ['proud', 'you'], 0),\n",
    "        ('father', ['work', 'harder'], 1),\n",
    "        ('father', ['love', 'you'], 0),\n",
    "        ('father', ['dont', 'quit'], 0)]\n",
    "\n",
    "speakers, vocab = set(), set()\n",
    "for i in range(len(data)):\n",
    "  speakers.add(data[i][0])\n",
    "  for j in range(CONTEXT_SIZE):\n",
    "    vocab.add(data[i][1][j])\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "speaker_to_ix = {speaker: i for i, speaker in enumerate(speakers)}\n",
    "\n",
    "\n",
    "class NGramLanguageModeler(nn.Module):\n",
    "    def __init__(self, speaker_size, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embeddings2 = nn.Embedding(speaker_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim+(context_size * embedding_dim), 128)\n",
    "        self.linear2 = nn.Linear(128, 1)\n",
    "    def forward(self, inputs):\n",
    "        speaker,sentence = inputs\n",
    "        sentence_embed = self.embeddings(sentence).view((1, -1))\n",
    "        speaker_embed = self.embeddings2(speaker).view((1, -1))\n",
    "        embeds_full = torch.cat((speaker_embed,sentence_embed), -1) \n",
    "        out = F.relu(self.linear1(embeds_full))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = torch.sigmoid(out)\n",
    "        return log_probs\n",
    "\n",
    "\n",
    "losses = []\n",
    "loss_function = nn.BCELoss()\n",
    "model = NGramLanguageModeler(len(speakers), len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for speaker, sentence, target in data:\n",
    "        word_idxs = [word_to_ix[w] for w in sentence]\n",
    "        word_idxs = torch.tensor(word_idxs, dtype=torch.long)\n",
    "        speaker_idxs = [speaker_to_ix[speaker]]\n",
    "        speaker_idxs = torch.tensor(speaker_idxs, dtype=torch.long)\n",
    "        model.zero_grad()\n",
    "        log_probs = model((speaker_idxs, word_idxs))\n",
    "        loss = loss_function(log_probs, torch.tensor([target], dtype=torch.float).resize_((1, 1)))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "\n",
    "\n",
    "print(losses)  # The loss decreased every iteration over the training data!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
